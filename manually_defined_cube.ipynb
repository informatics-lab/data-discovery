{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothysis\n",
    "\n",
    "Base on the assersion that:\n",
    "\n",
    "1. It is common that the scientests working with a data set know and understand the shape of the dataset\n",
    "2. Files in a data set are named such that filenames/paths can be derived for any given chunk of data\n",
    "\n",
    "It should be possiable to create a 'hyper' cube based purely on 'theoretical' data that is supplied by the data set curator.\n",
    "\n",
    "A question remains of how safe this is, if the some of the data is missing or curupt or the curator is wrong in their assumptions about the files is there a danger of being 'lied to' by the data or will the data access simply fail gracefully (or some other defined behaviour such as return NaN, None, Null, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data...\n",
    "Hear I'm just trying to understand what the data set I want to work with looks like. It is hoped that for most users they either know\n",
    "this already or someone else does this part for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91b4dccc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import netCDF4\n",
    "import glob\n",
    "import iris\n",
    "import os\n",
    "import warnings\n",
    "import cf_units \n",
    "import dask.array as da\n",
    "\n",
    "# For plotting later\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "plt.viridis()\n",
    "rcParams['figure.figsize'] = (25.0, 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 files e.g...\n",
      "/s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p10_196001-196912.nc\n",
      "/s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p10_197001-197912.nc\n",
      "/s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p10_198001-198912.nc\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/s3/informatics-eupheme/'\n",
    "sub_dir = data_dir + 'HadGEM3-A-N216/historical/tas/Amon/'\n",
    "sub_dir\n",
    "\n",
    "import glob\n",
    "files = sorted(glob.glob(sub_dir + '*.nc'))\n",
    "print(\"%s files e.g...\" % len(files))\n",
    "print('\\n'.join(files[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the files to see what is in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tas_Amon_HadGEM3-A-N216_historical_r1i1p10_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_196001-196912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_201001-201312.nc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([os.path.basename(f) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path /s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p10_196001-196912.nc\n"
     ]
    }
   ],
   "source": [
    "def describe(file):\n",
    "    print(\"file path %s\" % file)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for cube in iris.load_raw(file):\n",
    "            print(cube)\n",
    "            print('\\n\\n')\n",
    "\n",
    "describe(files[0])\n",
    "describe(files[20])\n",
    "describe(files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look to be files of just 'air_temperature' on the same grid. `time` always 120 steps. Let's look into the 'points' on that axis more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_201001-201312.nc' # 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_196001-196912.nc'\n",
    "file = os.path.join(data_dir, sub_dir, basename)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    cubes = iris.load_raw(file)\n",
    "assert len(cubes) == 1 # Belive true from above investigation\n",
    "\n",
    "cube = cubes[0]\n",
    "tcoord = cube.coord('time')\n",
    "print(\"given file name file %s\" % os.path.basename(file))\n",
    "print(\"times (%s) :\" % tcoord.units)\n",
    "print (' '.join(\"%s (%s)\" % (p, tcoord.units.num2date(p)) for p in cube.coord('time').points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the filename is `tas_Amon_HadGEM3-A-N216_historical_r1i1pP_YYYYMM-YYYYMM.nc` where YYYY MM are the start and stop dates in the file and are inclusive. P is the \"physics version\" as an int. Times in the file are for 12 months * 10 years = 120 timesteps. P is 1 to 15. \n",
    "\n",
    "Files start at `196001-` and end at `201001-` and inc by 10 years so `196001-196912`, `200001-200912`. The last file `201001-` is the exception and ends at `201312`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the metadata (mainly the coord points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNIT = 'days since 1960-01-01'\n",
    "VNAME = 'tas'\n",
    "\n",
    "# Convert a date (in 360 day calendar to a number in units days since 1960-01-01)\n",
    "def date_to_num(year, month, day):\n",
    "    syear, smonth, sday = TUNIT.rstrip().split(' ')[-1].split('-')\n",
    "    assert sday == smonth and int(sday) == 1\n",
    "    assert TUNIT.strip().startswith('days since')\n",
    "    start_year = int(syear)\n",
    "    return (year - start_year) * 360 + (month - 1 )* 30 + ( day -1)\n",
    "\n",
    "\n",
    "print(date_to_num(1969, 12, 16))# should be 3585\n",
    "print(date_to_num(1963, 6, 16))# 1245.0 \n",
    "print(date_to_num(1960, 1, 16))# 15.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = (1960, 1, 16)\n",
    "last_date = (2013, 12, 16)\n",
    "\n",
    "current = date_to_num(*first_date)\n",
    "time_coord_points=[]\n",
    "while current <= date_to_num(*last_date):\n",
    "    time_coord_points.append(current)\n",
    "    current+=30\n",
    "\n",
    "physics_coord_points = list(range(1,16))\n",
    "\n",
    "latitude_coord_points = cube.coord('latitude').points\n",
    "longitude_coord_points = cube.coord('longitude').points\n",
    "\n",
    "print(\"time_coord_points: %s ... %s\" % (time_coord_points[:5], time_coord_points[-5:]))\n",
    "print(\"physics_coord_points: %s\" % physics_coord_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the `time` coord points and `physics` coord points. We will use `latitude` and `longitude` as is from one of the files.\n",
    "\n",
    "We define a function that given a index in `(time, physics)` space returns the filepath of the NetCDF that contains the data, it also returns the shape of the data. Theoretical it should probably opporate over `latitude` and `longitude` but we ignore them as they don't effect the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_details(time_index, pysics_index):\n",
    "    time = cf_units.num2date(time_coord_points[time_index], TUNIT,cf_units.CALENDAR_STANDARD)\n",
    "    syear = str(time.year)[:3] + '0'\n",
    "    eyear = syear[:3] + '9' \n",
    "    shape = (120, 324, 432) \n",
    "\n",
    "    if syear == '2010':\n",
    "        eyear = '2013'\n",
    "        shape = (48, 324, 432) \n",
    "    \n",
    "    basename = \"tas_Amon_HadGEM3-A-N216_historical_r1i1p{physics}_{syear}01-{eyear}12.nc\".format(\n",
    "        syear=syear, eyear=eyear, physics=(pysics_index +1))\n",
    "    path =  os.path.join(data_dir, sub_dir, basename)\n",
    "    \n",
    "    return path, shape\n",
    "    \n",
    "file_details(len(time_coord_points)-1, 4)\n",
    "file_details(6, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our dimention defs and file look up\n",
    "This stage is a test for our work so far and isn't part of building the cube. Let's check for all points in `(time, physics)` space that we can find the file we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found=[]\n",
    "notfound=[]\n",
    "for p in range(0, len(physics_coord_points)):\n",
    "    for t in range(0, len(time_coord_points)):\n",
    "        file, _ = file_details(t, p)\n",
    "        if os.path.exists(file):\n",
    "            found.append(file)\n",
    "        else:\n",
    "            notfound.append(file)\n",
    "print(\"Found %s. Not found %s\" % (len(found), len(notfound)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our cube\n",
    "\n",
    "Two ways spring to mind:\n",
    "\n",
    "1. One go through all the files and create a `iris.fileformats.netcdf.NetCDFDataProxy` for each file, wrap with a dask array and then concat and stack those arrays in to one. \n",
    "2. Extend/implement an equivilant to `iris.fileformats.netcdf.NetCDFDataProxy` that looks up the path on the file based on the 'keys' passed in to `__getitem__`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastfile = None\n",
    "p_arrays = []\n",
    "dim_order = ['physics', 'time', 'lat', 'lon']\n",
    "for p in range(0, len(physics_coord_points)):\n",
    "    t_arrays = []\n",
    "    for t in range(0, len(time_coord_points)):\n",
    "      \n",
    "        file, shape = file_details(t, p)            \n",
    "        if file != lastfile:\n",
    "            data = iris.fileformats.netcdf.NetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                VNAME,\n",
    "                None)\n",
    "            data = da.from_array(data, data.shape)\n",
    "            t_arrays.append(data)\n",
    "        \n",
    "        lastfile = file\n",
    "    p_arrays.append(da.concatenate(t_arrays,0))\n",
    "    \n",
    "data = da.stack(p_arrays, 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(15* 648* 324* 432)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data access in the array\n",
    "data[9, 9, 8, 3].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a cube from our data array\n",
    "\n",
    "def points_to_coord(var_name, points, units=None, long_name=None, standard_name=None):\n",
    "    long_name = long_name if long_name else var_name\n",
    "    return iris.coords.DimCoord(\n",
    "        points=points,\n",
    "        standard_name=standard_name,\n",
    "        long_name=long_name,\n",
    "        var_name=var_name, units=units)\n",
    "\n",
    "\n",
    "\n",
    "dim_coords = [\n",
    "    points_to_coord('physics', physics_coord_points),\n",
    "    points_to_coord('time', time_coord_points, TUNIT),\n",
    "    points_to_coord('latitude', latitude_coord_points, 'degrees'),\n",
    "    points_to_coord('longitude', longitude_coord_points, 'degrees')\n",
    "]\n",
    "\n",
    "cube = iris.cube.Cube(\n",
    "        data=data,\n",
    "        standard_name='air_temperature',\n",
    "        long_name='air_temperature',\n",
    "        var_name='tas',\n",
    "        units = cf_units.Unit('K'),\n",
    "        dim_coords_and_dims=[(coord, i) for i, coord in enumerate(dim_coords)])\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the cube as a normal cube... plot, extract, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot some of that data\n",
    "qplt.contourf(cube[0,0,:,:], 25)\n",
    "\n",
    "# Add coastlines to the map created by contourf.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# BG want this demonstrated for a live TB size data set....\n",
    "# Their us case is accessing one point for all model runs for a give validity time, the below is equivilant(ish)\n",
    "print(cube[:,7,45,45].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_constraint = iris.Constraint(latitude=lambda cell: 49.8 < cell < 59.5, longitude=lambda cell: -10.9 < cell < 2.0)\n",
    "uk_mean = (cube.extract(uk_constraint).collapsed('physics', iris.analysis.MEAN)\n",
    "                                    .collapsed('latitude', iris.analysis.MEAN)\n",
    "                                    .collapsed('longitude', iris.analysis.MEAN))\n",
    "uk_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get advantages if we use a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "import distributed\n",
    "cluster = KubeCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(cluster.scheduler_address)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "print(cube[:,6,45,45].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplt.plot(uk_mean)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
