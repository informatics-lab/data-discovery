{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this note book I intend to explore methods/functions/classes that could be created to make creating a 'hyper cube' quick and easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files to work with\n",
    "import glob\n",
    "data_dir = '/s3/informatics-eupheme/'\n",
    "sub_dir = data_dir + 'HadGEM3-A-N216/historical/tas/Amon/'\n",
    "files = sorted(glob.glob(sub_dir + '*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods to define coords\n",
    "import iris\n",
    "from collections import namedtuple\n",
    "import cf_units\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "def coord_from_cube(cube, coord_name, redefined_points=None,redefined_bounds=None):\n",
    "    coord = cube.coord(coord_name).copy(points=redefined_points, bounds=redefined_bounds)\n",
    "    return coord\n",
    "\n",
    "def coord_from_points(points, units='1', standard_name=None, long_name=None, var_name=None, bounds=None):\n",
    "    return iris.coords.Coord(points, long_name=long_name, units=units, standard_name=standard_name, bounds=bounds)\n",
    "\n",
    "CoordAgg = namedtuple('CoordAgg', ['type', 'coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sample_cube = iris.load_raw(files[5])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'latitude', 'longitude']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.name() for c in sample_cube.dim_coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = coord_from_cube(sample_cube, 'latitude')\n",
    "longitude = coord_from_cube(sample_cube,'longitude')\n",
    "time = coord_from_cube(sample_cube,'time',range(15,19425+1,30)) # 1960-01-15 to 2013-12-15 in 'days since 1960-01-01' on 360 day calander one month (30 day) steps\n",
    "pyhsics = coord_from_points(range(1, 15+1), long_name='physics')\n",
    "\n",
    "coords = [CoordAgg('stack', pyhsics), CoordAgg('concatenate', time), CoordAgg('const', latitude), CoordAgg('const', longitude)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "Stack [dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>] <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "Stack [dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>] <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "item <class 'dask.array.core.Array'>\n",
      "Stack [dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>, dask.array<array, shape=(1,), dtype=<U5, chunksize=(1,)>] <class 'dask.array.core.Array'>\n",
      "Stack [dask.array<stack, shape=(3, 1), dtype=<U5, chunksize=(1, 1)>, dask.array<stack, shape=(3, 1), dtype=<U5, chunksize=(1, 1)>, dask.array<stack, shape=(3, 1), dtype=<U5, chunksize=(1, 1)>] <class 'dask.array.core.Array'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[['(1,1)'],\n",
       "        ['(1,2)'],\n",
       "        ['(1,3)']],\n",
       "\n",
       "       [['(2,1)'],\n",
       "        ['(2,2)'],\n",
       "        ['(2,3)']],\n",
       "\n",
       "       [['(3,1)'],\n",
       "        ['(3,2)'],\n",
       "        ['(3,3)']]], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VNAME = \"tas\"\n",
    "\n",
    "def stack_over(coord, getter ):\n",
    "    def stack(*args):\n",
    "        to_stack = list(getter(*args, p) for p in coord.points)\n",
    "        print(\"Stack\", to_stack, type(to_stack[0]))\n",
    "        return da.stack(to_stack)\n",
    "    return stack\n",
    "\n",
    "def concatenate_over(coord, getter ):\n",
    "    def concatenate(*args):\n",
    "        to_concatenate = list(getter(*args, p) for p in coord.points)\n",
    "        print(\"concatenate\", to_concatenate, type(to_concatenate[0]))\n",
    "        return da.concatenate(to_concatenate)\n",
    "    return concatenate\n",
    "\n",
    "def file_details(pysics_index, time_index): \n",
    "    data_dir = '/s3/informatics-eupheme/'\n",
    "    sub_dir = data_dir + 'HadGEM3-A-N216/historical/tas/Amon/'\n",
    "    time = cf_units.num2date(time_coord_points[time_index], TUNIT,cf_units.CALENDAR_STANDARD)\n",
    "    syear = str(time.year)[:3] + '0'\n",
    "    eyear = syear[:3] + '9' \n",
    "    shape = (120, 324, 432) \n",
    "    if syear == '2010':\n",
    "        eyear = '2013'\n",
    "        shape = (48, 324, 432) \n",
    "    basename = \"tas_Amon_HadGEM3-A-N216_historical_r1i1p{physics}_{syear}01-{eyear}12.nc\".format(\n",
    "        syear=syear, eyear=eyear, physics=(pysics_index +1))\n",
    "    path =  os.path.join(data_dir, sub_dir, basename)\n",
    "    return path, shape\n",
    "\n",
    "\n",
    "def caching_getter():\n",
    "    file_details(p, t)\n",
    "\n",
    "def getter(p, t):\n",
    "    file, shape = file_details(p, t)\n",
    "    \n",
    "\n",
    "def cached_file_to_array(cache, file, shape):\n",
    "    data = cache.get(file, None)\n",
    "    if not data:\n",
    "        data = file_to_array(file, shape)\n",
    "    cache[file] = data\n",
    "    return data \n",
    "    \n",
    "def file_to_array(file, shape):\n",
    "    data = iris.fileformats.netcdf.NetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                VNAME,\n",
    "                None)\n",
    "    data = da.from_array(data, data.shape)\n",
    " \n",
    "\n",
    "\n",
    "# def dim_concatenate(coord, getter ):\n",
    "#     to_concatenate = list(getter(p) for p in coord.points)\n",
    "#     return da.concatenate(to_stack)\n",
    "\n",
    "class Persist(object)\n",
    "    def get_file(physics, time):\n",
    "        item = da.from_array(np.array([\"(%s,%s)\"%(physics, time)]), 1)\n",
    "        print(\"item\", type(item))\n",
    "        return item, item.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_file(physics, time):\n",
    "    item = da.from_array(np.array([\"(%s,%s)\"%(physics, time)]), 1)\n",
    "    print(\"item\", type(item))\n",
    "    return item, item.shape\n",
    "\n",
    "pcoord = namedtuple('Coord',['points'])([1,2,3])\n",
    "tcoord = namedtuple('Coord',['points'])([1,2,3])\n",
    "\n",
    "the_agg = dim_stack(pcoord, \n",
    "              dim_stack(tcoord, get_file))()\n",
    "the_agg.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(0, len(physics)):\n",
    "    for t in range(0, len(time)):\n",
    "        file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'physics' is not a valid standard_name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ba84516e11ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlongitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_from_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_cube\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'longitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_from_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_cube\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19425\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1960-01-15 to 2013-12-15 in 'days since 1960-01-01' on 360 day calander one month (30 day) steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpyhsics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_from_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'physics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c3b952b23b84>\u001b[0m in \u001b[0;36mcoord_from_points\u001b[0;34m(points, units, standard_name, long_name, var_name, bounds)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcoord_from_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlong_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstandard_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/iris/coords.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, points, standard_name, long_name, var_name, units, bounds, attributes, coord_system)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \"\"\"\n\u001b[1;32m    443\u001b[0m         \u001b[0;31m#: CF standard name of the quantity that the coordinate represents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m#: Descriptive name of the coordinate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/iris/_cube_coord_common.py\u001b[0m in \u001b[0;36mstandard_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standard_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r is not a valid standard_name'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'physics' is not a valid standard_name"
     ]
    }
   ],
   "source": [
    "def builder(coords, file_finder):\n",
    "    constants = []\n",
    "    while coords[-1].type == 'const':\n",
    "        coords = coords[:-1]\n",
    "    \n",
    "    top_level_coord, *other_coords = coords \n",
    "    for point in top_level_coord.points:\n",
    "        if\n",
    "        \n",
    "def build(coords, file_finder):\n",
    "    top_level_coord, *other_coords = coords \n",
    "    for point in top_level_coord.points:\n",
    "        if len(other_coords) > 0:\n",
    "            arrays = build(coords)\n",
    "            if(top_level_coord.type == 'stack'):\n",
    "                 da.stack(p_arrays, 0)\n",
    "        else:\n",
    "            file, shape = file_details(t, p)\n",
    "            data = iris.fileformats.netcdf.NetCDFDataProxy(\n",
    "                            shape, 'float32', file, VNAME, None)\n",
    "            data = da.from_array(data, data.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "class NetCDFDataProxy(object):\n",
    "    \"\"\"A reference to the data payload of a single NetCDF file variable.\"\"\"\n",
    "\n",
    "    __slots__ = ('shape', 'dtype', 'path', 'variable_name', 'fill_value')\n",
    "\n",
    "    def __init__(self, shape, dtype, path, variable_name, fill_value):\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self.path = path\n",
    "        self.variable_name = variable_name\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return len(self.shape)\n",
    "\n",
    "    def __getitem__(self, keys):\n",
    "        print('__getitem__', keys)\n",
    "        dataset = netCDF4.Dataset(self.path)\n",
    "        try:\n",
    "            variable = dataset.variables[self.variable_name]\n",
    "            # Get the NetCDF variable data and slice.\n",
    "            var = variable[keys]\n",
    "        finally:\n",
    "            dataset.close()\n",
    "        return np.asanyarray(var)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt = '<{self.__class__.__name__} shape={self.shape}' \\\n",
    "              ' dtype={self.dtype!r} path={self.path!r}' \\\n",
    "              ' variable_name={self.variable_name!r}>'\n",
    "        return fmt.format(self=self)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {attr: getattr(self, attr) for attr in self.__slots__}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        for key, value in six.iteritems(state):\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "\n",
    "file, shape = ('/s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p2_196001-196912.nc', (120, 324, 432))\n",
    "data = NetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                'tas',\n",
    "                None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear from the above but the the problem become hard if you work from the top down, i.e. take the desierd hypercube and try work out the path through the aggrigations down to the individual files you might need. I think the problem will be easier if working the other way. Start with the individual files and 'build them up' in to a hypercube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tas_Amon_HadGEM3-A-N216_historical_r1i1p1_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p2_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p3_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p4_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p5_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p6_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p7_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p8_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p9_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p10_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p11_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p12_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p13_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p14_201001-201312.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_197001-197912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_198001-198912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_199001-199912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_200001-200912.nc',\n",
       " 'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_201001-201312.nc']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def date_to_num(year, month, day):\n",
    "    TUNIT = 'days since 1960-01-01'\n",
    "    syear, smonth, sday = TUNIT.rstrip().split(' ')[-1].split('-')\n",
    "    assert sday == smonth and int(sday) == 1\n",
    "    assert TUNIT.strip().startswith('days since')\n",
    "    start_year = int(syear)\n",
    "    return (year - start_year) * 360 + (month - 1 )* 30 + ( day -1)\n",
    "\n",
    "def file_to_coords(file):\n",
    "    Dim = namedtuple('Dim', [ 'name','points','as_dim_in_file'])\n",
    "    match = re.match(\"tas_Amon_HadGEM3-A-N216_historical_r1i1p([0-9]+)_([0-9]{6})-([0-9]{6}).nc\", file)\n",
    "    physics, start, end = match.groups()\n",
    "    physics = int(physics)\n",
    "    syear, smonth = int(start[:4]), int(start[4:])\n",
    "    eyear, emonth = int(end[:4]), int(end[4:])\n",
    "    \n",
    "    physics = Dim('physics', physics, False)\n",
    "    time = Dim('time', list(range(date_to_num(syear, smonth, 15), date_to_num(eyear, emonth, 15)+1, 30)), True)\n",
    "    lat = Dim('latitude', latitude.points, True)\n",
    "    lon = Dim('longitude', longitude.points, True)\n",
    "\n",
    "    return (physics, time, lat, lon)\n",
    "\n",
    "def file_names():\n",
    "    files = []\n",
    "    for physics in range(1,15+1):\n",
    "        for decade in range (1970, 2010+1, 10):\n",
    "            start = decade\n",
    "            end = decade+9 if decade < 2010 else 2013\n",
    "            rangestr = \"%d01-%d12\" %(start, end)\n",
    "            files.append('tas_Amon_HadGEM3-A-N216_historical_r1i1p%d_%d01-%d12.nc' % (physics, start, end))\n",
    "    return files\n",
    "#     decades = 1970\n",
    "#     'tas_Amon_HadGEM3-A-N216_historical_r1i1p15_201001-201312.nc',\n",
    "#     'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_196001-196912.nc',\n",
    "#     'tas_Amon_HadGEM3-A-N216_historical_r1i1p1_197001-197912.nc',\n",
    "\n",
    "file ='tas_Amon_HadGEM3-A-N216_historical_r1i1p2_196001-196912.nc'\n",
    "file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggriableNetCDFProxy(iris.fileformats.netcdf.NetCDFDataProxy):\n",
    "    def __init__(self, dims, path, variable_name):\n",
    "        shape = [len(d.points) for d in dims if d.as_dim_in_file]\n",
    "        self.dims = dims\n",
    "        super().__init__(shape, 'float32', path, variable_name, None)\n",
    "    \n",
    "     \n",
    "    \n",
    "def build_ndarray(var, coords_and_file):\n",
    "    coord, file = coords_and_file\n",
    "    NetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                var,\n",
    "                None)\n",
    "    \n",
    "    \n",
    "data = AggriableNetCDFProxy(file_to_coords(file), file, 'tas')\n",
    "ddata = da.from_array(data, data.shape)\n",
    "\n",
    "def concat(dim_name, aggriables):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "# AggStack('physics', \n",
    "#          agg_concat('time',\n",
    "#                 build_ndarray(\n",
    "#                   file_to_coords(file) for file in file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 324, 432]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Array.var of dask.array<array, shape=(120, 324, 432), dtype=float32, chunksize=(120, 324, 432)>>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddata.var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
