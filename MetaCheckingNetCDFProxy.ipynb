{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the simplest code to generate the 'hyper cube' from 'HadGEM3-A-N216/historical/tas/Amon/' data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_temperature / (K)               (physics: 15; time: 648; latitude: 324; longitude: 432)\n",
      "     Dimension coordinates:\n",
      "          physics                           x         -              -               -\n",
      "          time                              -         x              -               -\n",
      "          latitude                          -         -              x               -\n",
      "          longitude                         -         -              -               x\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import iris\n",
    "import warnings \n",
    "import cf_units\n",
    "import os\n",
    "from dask import array as da\n",
    "\n",
    "TUNIT = 'days since 1960-01-01'\n",
    "VNAME = 'tas'\n",
    "\n",
    "data_dir = '/s3/informatics-eupheme/'\n",
    "sub_dir = data_dir + 'HadGEM3-A-N216/historical/tas/Amon/'\n",
    "sub_dir\n",
    "\n",
    "files = sorted(glob.glob(sub_dir + '*.nc'))\n",
    "\n",
    "\n",
    "time_coord_points=list(range(15,19425+1,30))\n",
    "physics_coord_points = list(range(1,16))\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for cube in iris.load_raw(files[0]):\n",
    "            latitude_coord_points = cube.coord('latitude').points\n",
    "            longitude_coord_points = cube.coord('longitude').points\n",
    "\n",
    "\n",
    "def file_details(time_index, pysics_index):\n",
    "    time = cf_units.num2date(time_coord_points[time_index], TUNIT,cf_units.CALENDAR_STANDARD)\n",
    "    syear = str(time.year)[:3] + '0'\n",
    "    eyear = syear[:3] + '9' \n",
    "    shape = (120, 324, 432) \n",
    "\n",
    "    if syear == '2010':\n",
    "        eyear = '2013'\n",
    "        shape = (48, 324, 432) \n",
    "    \n",
    "    basename = \"tas_Amon_HadGEM3-A-N216_historical_r1i1p{physics}_{syear}01-{eyear}12.nc\".format(\n",
    "        syear=syear, eyear=eyear, physics=(pysics_index +1))\n",
    "    path =  os.path.join(data_dir, sub_dir, basename)\n",
    "    \n",
    "    return path, shape\n",
    "\n",
    "\n",
    "\n",
    "lastfile = None\n",
    "p_arrays = []\n",
    "dim_order = ['physics', 'time', 'lat', 'lon']\n",
    "for p in range(0, len(physics_coord_points)):\n",
    "    t_arrays = []\n",
    "    for t in range(0, len(time_coord_points)):\n",
    "      \n",
    "        file, shape = file_details(t, p)            \n",
    "        if file != lastfile:\n",
    "            data = iris.fileformats.netcdf.NetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                VNAME,\n",
    "                None)\n",
    "            data = da.from_array(data, data.shape)\n",
    "            t_arrays.append(data)\n",
    "        \n",
    "        lastfile = file\n",
    "    p_arrays.append(da.concatenate(t_arrays,0))\n",
    "    \n",
    "data = da.stack(p_arrays, 0)\n",
    "data.shape\n",
    "\n",
    "\n",
    "# Build a cube from our data array\n",
    "\n",
    "def points_to_coord(var_name, points, units=None, long_name=None, standard_name=None):\n",
    "    long_name = long_name if long_name else var_name\n",
    "    return iris.coords.DimCoord(\n",
    "        points=points,\n",
    "        standard_name=standard_name,\n",
    "        long_name=long_name,\n",
    "        var_name=var_name, units=units)\n",
    "\n",
    "\n",
    "\n",
    "dim_coords = [\n",
    "    points_to_coord('physics', physics_coord_points),\n",
    "    points_to_coord('time', time_coord_points, TUNIT),\n",
    "    points_to_coord('latitude', latitude_coord_points, 'degrees'),\n",
    "    points_to_coord('longitude', longitude_coord_points, 'degrees')\n",
    "]\n",
    "\n",
    "cube = iris.cube.Cube(\n",
    "        data=data,\n",
    "        standard_name='air_temperature',\n",
    "        long_name='air_temperature',\n",
    "        var_name='tas',\n",
    "        units = cf_units.Unit('K'),\n",
    "        dim_coords_and_dims=[(coord, i) for i, coord in enumerate(dim_coords)])\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That code uses the iris `NetCDFDataProxy` I want to create a new version of this that will perform some safety checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "#netCDF4.Dataset('/this/no/real.nc')\n",
    "\n",
    "dataset= netCDF4.Dataset(files[0])\n",
    "variable = dataset.variables['tas']\n",
    "\n",
    "\n",
    "dataset variable.missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy.ma as ma\n",
    "class CheckingNetCDFDataProxy(object):\n",
    "    \"\"\"A reference to the data payload of a single NetCDF file variable.\"\"\"\n",
    "\n",
    "    __slots__ = ('shape', 'dtype', 'path', 'variable_name', 'fill_value', 'safety_check_done', 'fatal_fail')\n",
    "\n",
    "    def __init__(self, shape, dtype, path, variable_name, fill_value=None, do_safety_check=False):\n",
    "        self.safety_check_done = do_safety_check\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "        self.path = path\n",
    "        self.variable_name = variable_name\n",
    "        self.fill_value = fill_value\n",
    "        self.fatal_fail = None\n",
    "\n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return len(self.shape)\n",
    "\n",
    "    def check(self):\n",
    "        try:\n",
    "            dataset = netCDF4.Dataset(self.path)\n",
    "        except OSError:\n",
    "            self.fatal_fail = \"no such file %s\" % self.path\n",
    "            self.safety_check_done = True\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            variable = dataset.variables[self.variable_name]\n",
    "        except KeyError:\n",
    "            self.fatal_fail = \"no variable %s in file %s\" % (self.variable_name, self.path)\n",
    "            self.safety_check_done = True\n",
    "            return\n",
    "\n",
    "        if variable.shape != self.shape:\n",
    "            self.fatal_fail = \"Shape of data %s doesn't match expected %s\" %(variable.shape, self.shape)\n",
    "            self.safety_check_done = True\n",
    "            return\n",
    "        \n",
    "        # TODO check variables???\n",
    "        \n",
    "        self.safety_check_done = True\n",
    "        \n",
    "        \n",
    "    def  _null_data(self, keys):\n",
    "#         if not self.fill_value:\n",
    "#             raise AttributeError(\"Can not create null data when fill value not known.\")\n",
    "#         else:\n",
    "#             return (np.ones(self.shape) * self.fill_value)[keys]\n",
    "        return ma.masked_all(self.shape)[keys]\n",
    "        \n",
    "    def __getitem__(self, keys):\n",
    "        print('__getitem__', keys)\n",
    "        \n",
    "        if not self.safety_check_done:\n",
    "            self.check()\n",
    "            \n",
    "        if self.fatal_fail:\n",
    "            return self._null_data(keys)\n",
    "            \n",
    "        try:\n",
    "            dataset = netCDF4.Dataset(self.path)\n",
    "            variable = dataset.variables[self.variable_name]\n",
    "            # Get the NetCDF variable data and slice.\n",
    "            var = variable[keys]\n",
    "        finally:\n",
    "            if dataset:\n",
    "                dataset.close()\n",
    "        return np.asanyarray(var)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt = '<{self.__class__.__name__} shape={self.shape}' \\\n",
    "              ' dtype={self.dtype!r} path={self.path!r}' \\\n",
    "              ' variable_name={self.variable_name!r}>'\n",
    "        return fmt.format(self=self)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {attr: getattr(self, attr) for attr in self.__slots__}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        for key, value in six.iteritems(state):\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "\n",
    "file, shape = ('/s3/informatics-eupheme/HadGEM3-A-N216/historical/tas/Amon/tas_Amon_HadGEM3-A-N216_historical_r1i1p2_196001-196912.nc', (120, 324, 432))\n",
    "data_none = CheckingNetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                'pas',\n",
    "                None)\n",
    "\n",
    "data = CheckingNetCDFDataProxy(\n",
    "                shape,\n",
    "                'float32',\n",
    "                file,\n",
    "                'tas',\n",
    "                None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 µs\n",
      "__getitem__ (slice(10, 30, None), 4, slice(None, None, None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        ...,\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --],\n",
       "        [--, --, --, ..., --, --, --]],\n",
       "  mask=[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "  fill_value=1e+20,\n",
       "  dtype=float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "data_none[10:30, 4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 µs\n",
      "__getitem__ (slice(10, 30, None), 4, slice(None, None, None))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[240.61377, 240.57666, 240.5581 , ..., 240.75854, 240.71289,\n",
       "        240.6643 ],\n",
       "       [249.70068, 249.65454, 249.60376, ..., 249.8562 , 249.80371,\n",
       "        249.75952],\n",
       "       [250.38647, 250.33862, 250.29395, ..., 250.51807, 250.47852,\n",
       "        250.42749],\n",
       "       ...,\n",
       "       [224.87036, 224.8186 , 224.74658, ..., 225.0979 , 225.02539,\n",
       "        224.97656],\n",
       "       [226.07153, 226.01758, 225.95508, ..., 226.2688 , 226.20068,\n",
       "        226.14502],\n",
       "       [214.76855, 214.68555, 214.60742, ..., 215.     , 214.93945,\n",
       "        214.8645 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "data[10:30, 4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
